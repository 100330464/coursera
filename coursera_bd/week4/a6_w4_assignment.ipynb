{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the final project of “Apache Spark for Scalable Machine Learning on BigData”. In this assignment you’ll analyze a real-world dataset and apply machine learning on it using Apache Spark. \n",
    "\n",
    "In order to pass, you need to implement some code (basically replace the parts marked with $$) and finally answer a quiz on the Coursera platform.\n",
    "\n",
    "Let’s start by downloading the dataset and creating a dataframe. This dataset can be found on DAX, the IBM Data Asset Exchange and can be downloaded for free.\n",
    "\n",
    "https://developer.ibm.com/exchanges/data/all/jfk-weather-data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-15 19:24:40--  http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n",
      "Resolving max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.193\n",
      "Connecting to max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.193|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2575759 (2.5M) [application/x-tar]\n",
      "Saving to: 'jfk_weather.tar.gz'\n",
      "\n",
      "jfk_weather.tar.gz  100%[===================>]   2.46M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-08-15 19:24:40 (36.7 MB/s) - 'jfk_weather.tar.gz' saved [2575759/2575759]\n",
      "\n",
      "./._jfk_weather.csv\n",
      "jfk_weather.csv\n"
     ]
    }
   ],
   "source": [
    "# delete files from previous runs\n",
    "!rm -f jfk_weather*\n",
    "\n",
    "# download the file containing the data in CSV format\n",
    "!wget http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n",
    "\n",
    "# extract the data\n",
    "!tar xvfz jfk_weather.tar.gz\n",
    "    \n",
    "# create a dataframe out of it by using the first row as field names and trying to infer a schema based on contents\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('jfk_weather.csv')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains some null values, therefore schema inference didn’t work properly for all columns, in addition, a column contained trailing characters, so we need to clean up the data set first. This is a normal task in any data science project since your data is never clean, don’t worry if you don’t understand all code, you won’t be asked about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate, col\n",
    "\n",
    "df_cleaned = df \\\n",
    "    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n",
    "    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n",
    "    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n",
    "    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n",
    "    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n",
    "    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\")) \\\n",
    "\n",
    "df_cleaned =   df_cleaned \\\n",
    "                    .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n",
    "                    .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n",
    "                    .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n",
    "                    .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \\\n",
    "\n",
    "df_filtered = df_cleaned.filter(\"\"\"\n",
    "    HOURLYWindSpeed <> 0\n",
    "    and HOURLYWindDirection <> 0\n",
    "    and HOURLYStationPressure <> 0\n",
    "    and HOURLYPressureTendency <> 0\n",
    "    and HOURLYPressureTendency <> 0\n",
    "    and HOURLYPrecip <> 0\n",
    "    and HOURLYRelativeHumidity <> 0\n",
    "    and HOURLYDRYBULBTEMPC <> 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|HOURLYDRYBULBTEMPC|\n",
      "+------------------+\n",
      "|             -10.6|\n",
      "|              12.8|\n",
      "|              29.4|\n",
      "|               8.3|\n",
      "|              20.0|\n",
      "|              16.7|\n",
      "|              15.0|\n",
      "|               6.1|\n",
      "|              22.8|\n",
      "|               4.4|\n",
      "|               1.7|\n",
      "|             -10.0|\n",
      "|              14.4|\n",
      "|              -1.7|\n",
      "|                 *|\n",
      "|              24.4|\n",
      "|              25.0|\n",
      "|              19.4|\n",
      "|              11.1|\n",
      "|               0.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- STATION_NAME: string (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- REPORTTPYE: string (nullable = true)\n",
      " |-- HOURLYSKYCONDITIONS: string (nullable = true)\n",
      " |-- HOURLYVISIBILITY: string (nullable = true)\n",
      " |-- HOURLYPRSENTWEATHERTYPE: string (nullable = true)\n",
      " |-- HOURLYDRYBULBTEMPF: string (nullable = true)\n",
      " |-- HOURLYDRYBULBTEMPC: string (nullable = true)\n",
      " |-- HOURLYWETBULBTEMPF: string (nullable = true)\n",
      " |-- HOURLYWETBULBTEMPC: string (nullable = true)\n",
      " |-- HOURLYDewPointTempF: string (nullable = true)\n",
      " |-- HOURLYDewPointTempC: string (nullable = true)\n",
      " |-- HOURLYRelativeHumidity: string (nullable = true)\n",
      " |-- HOURLYWindSpeed: double (nullable = true)\n",
      " |-- HOURLYWindDirection: double (nullable = true)\n",
      " |-- HOURLYWindGustSpeed: integer (nullable = true)\n",
      " |-- HOURLYStationPressure: double (nullable = true)\n",
      " |-- HOURLYPressureTendency: integer (nullable = true)\n",
      " |-- HOURLYPressureChange: string (nullable = true)\n",
      " |-- HOURLYSeaLevelPressure: string (nullable = true)\n",
      " |-- HOURLYPrecip: double (nullable = true)\n",
      " |-- HOURLYAltimeterSetting: string (nullable = true)\n",
      " |-- DAILYMaximumDryBulbTemp: integer (nullable = true)\n",
      " |-- DAILYMinimumDryBulbTemp: integer (nullable = true)\n",
      " |-- DAILYAverageDryBulbTemp: integer (nullable = true)\n",
      " |-- DAILYDeptFromNormalAverageTemp: double (nullable = true)\n",
      " |-- DAILYAverageRelativeHumidity: integer (nullable = true)\n",
      " |-- DAILYAverageDewPointTemp: integer (nullable = true)\n",
      " |-- DAILYAverageWetBulbTemp: integer (nullable = true)\n",
      " |-- DAILYHeatingDegreeDays: integer (nullable = true)\n",
      " |-- DAILYCoolingDegreeDays: integer (nullable = true)\n",
      " |-- DAILYSunrise: integer (nullable = true)\n",
      " |-- DAILYSunset: integer (nullable = true)\n",
      " |-- DAILYWeather: string (nullable = true)\n",
      " |-- DAILYPrecip: string (nullable = true)\n",
      " |-- DAILYSnowfall: string (nullable = true)\n",
      " |-- DAILYSnowDepth: string (nullable = true)\n",
      " |-- DAILYAverageStationPressure: double (nullable = true)\n",
      " |-- DAILYAverageSeaLevelPressure: double (nullable = true)\n",
      " |-- DAILYAverageWindSpeed: double (nullable = true)\n",
      " |-- DAILYPeakWindSpeed: integer (nullable = true)\n",
      " |-- PeakWindDirection: integer (nullable = true)\n",
      " |-- DAILYSustainedWindSpeed: integer (nullable = true)\n",
      " |-- DAILYSustainedWindDirection: integer (nullable = true)\n",
      " |-- MonthlyMaximumTemp: double (nullable = true)\n",
      " |-- MonthlyMinimumTemp: double (nullable = true)\n",
      " |-- MonthlyMeanTemp: double (nullable = true)\n",
      " |-- MonthlyAverageRH: string (nullable = true)\n",
      " |-- MonthlyDewpointTemp: string (nullable = true)\n",
      " |-- MonthlyWetBulbTemp: string (nullable = true)\n",
      " |-- MonthlyAvgHeatingDegreeDays: integer (nullable = true)\n",
      " |-- MonthlyAvgCoolingDegreeDays: integer (nullable = true)\n",
      " |-- MonthlyStationPressure: double (nullable = true)\n",
      " |-- MonthlySeaLevelPressure: double (nullable = true)\n",
      " |-- MonthlyAverageWindSpeed: double (nullable = true)\n",
      " |-- MonthlyTotalSnowfall: string (nullable = true)\n",
      " |-- MonthlyDeptFromNormalMaximumTemp: double (nullable = true)\n",
      " |-- MonthlyDeptFromNormalMinimumTemp: double (nullable = true)\n",
      " |-- MonthlyDeptFromNormalAverageTemp: double (nullable = true)\n",
      " |-- MonthlyDeptFromNormalPrecip: string (nullable = true)\n",
      " |-- MonthlyTotalLiquidPrecip: string (nullable = true)\n",
      " |-- MonthlyGreatestPrecip: string (nullable = true)\n",
      " |-- MonthlyGreatestPrecipDate: string (nullable = true)\n",
      " |-- MonthlyGreatestSnowfall: string (nullable = true)\n",
      " |-- MonthlyGreatestSnowfallDate: string (nullable = true)\n",
      " |-- MonthlyGreatestSnowDepth: string (nullable = true)\n",
      " |-- MonthlyGreatestSnowDepthDate: integer (nullable = true)\n",
      " |-- MonthlyDaysWithGT90Temp: integer (nullable = true)\n",
      " |-- MonthlyDaysWithLT32Temp: integer (nullable = true)\n",
      " |-- MonthlyDaysWithGT32Temp: integer (nullable = true)\n",
      " |-- MonthlyDaysWithLT0Temp: integer (nullable = true)\n",
      " |-- MonthlyDaysWithGT001Precip: string (nullable = true)\n",
      " |-- MonthlyDaysWithGT010Precip: string (nullable = true)\n",
      " |-- MonthlyDaysWithGT1Snow: integer (nullable = true)\n",
      " |-- MonthlyMaxSeaLevelPressureValue: string (nullable = true)\n",
      " |-- MonthlyMaxSeaLevelPressureDate: integer (nullable = true)\n",
      " |-- MonthlyMaxSeaLevelPressureTime: integer (nullable = true)\n",
      " |-- MonthlyMinSeaLevelPressureValue: string (nullable = true)\n",
      " |-- MonthlyMinSeaLevelPressureDate: integer (nullable = true)\n",
      " |-- MonthlyMinSeaLevelPressureTime: integer (nullable = true)\n",
      " |-- MonthlyTotalHeatingDegreeDays: string (nullable = true)\n",
      " |-- MonthlyTotalCoolingDegreeDays: string (nullable = true)\n",
      " |-- MonthlyDeptFromNormalHeatingDD: string (nullable = true)\n",
      " |-- MonthlyDeptFromNormalCoolingDD: string (nullable = true)\n",
      " |-- MonthlyTotalSeasonToDateHeatingDD: integer (nullable = true)\n",
      " |-- MonthlyTotalSeasonToDateCoolingDD: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.select('HOURLYDRYBULBTEMPC').distinct().show()\n",
    "df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the value of one column based of some others. It is sometimes helpful to print a correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.2247899 , -0.28451144],\n",
       "       [ 0.2247899 ,  1.        , -0.1192573 ],\n",
       "       [-0.28451144, -0.1192573 ,  1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYWindDirection\",\"HOURLYStationPressure\"],\n",
    "                                  outputCol=\"features\")\n",
    "df_pipeline = vectorAssembler.transform(df_filtered)\n",
    "from pyspark.ml.stat import Correlation\n",
    "Correlation.corr(df_pipeline,\"features\").head()[0].toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, HOURLYWindSpeed and HOURLYWindDirection correlate with 0.2247899 whereas HOURLYWindSpeed  and HOURLYStationPressure correlate with -0.28451144, this is a good sign if we want to predict HOURLYWindSpeed from HOURLYWindDirection and HOURLYStationPressure.\n",
    "Since this is supervised learning, let’s split our data into train (80%) and test (20%) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df_filtered.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can re-use our feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindDirection\",\"ELEVATION\",\"HOURLYStationPressure\"],\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "bucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol=\"HOURLYWindDirection\", outputCol=\"HOURLYWindDirectionBucketized\")\n",
    "encoder = OneHotEncoder(inputCol=\"HOURLYWindDirectionBucketized\", outputCol=\"HOURLYWindDirectionOHE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "lr = LinearRegression(labelCol=\"HOURLYWindSpeed\", maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=10)\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer,gbt])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=10)\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindDirectionOHE\",\"ELEVATION\",\"HOURLYStationPressure\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,encoder,vectorAssembler,normalizer,gbt])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\n",
    "#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6211180124223602"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n",
    "    \n",
    "mcEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=20)\n",
    "#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6801242236024845"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n",
    "    \n",
    "mcEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n",
    "#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,gbt])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6459627329192547"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n",
    "    \n",
    "mcEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
